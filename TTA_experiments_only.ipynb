{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RachelRamirez/tta/blob/main/TTA_experiments_only.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Comparison of 4 Probabilistics Techniques** "
      ],
      "metadata": {
        "id": "bTsV0gZz-tQ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Traditional CNN (baseline),  2 \"Bayesian\" CNNs, and Test-Time Augmentations on CIFAR-9 classes and one novel class"
      ],
      "metadata": {
        "id": "qU2WftRnx2r5"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-aR_ls1Ci-nl"
      },
      "source": [
        "# Cifar10 classification case study with novel class\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Intro"
      ],
      "metadata": {
        "id": "iM3VBrzs1N88"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Load Neptune to Track Experiments"
      ],
      "metadata": {
        "id": "iEOTY2UNLtQO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install neptune-client"
      ],
      "metadata": {
        "id": "QxH02s_XLs_G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3088216-f8f8-440d-b015-0bd00da40093"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: neptune-client in /usr/local/lib/python3.8/dist-packages (0.16.15)\n",
            "Requirement already satisfied: bravado<12.0.0,>=11.0.0 in /usr/local/lib/python3.8/dist-packages (from neptune-client) (11.0.3)\n",
            "Requirement already satisfied: requests>=2.20.0 in /usr/local/lib/python3.8/dist-packages (from neptune-client) (2.25.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.8/dist-packages (from neptune-client) (5.4.8)\n",
            "Requirement already satisfied: PyJWT in /usr/local/lib/python3.8/dist-packages (from neptune-client) (2.6.0)\n",
            "Requirement already satisfied: Pillow>=1.1.6 in /usr/local/lib/python3.8/dist-packages (from neptune-client) (7.1.2)\n",
            "Requirement already satisfied: boto3>=1.16.0 in /usr/local/lib/python3.8/dist-packages (from neptune-client) (1.26.45)\n",
            "Requirement already satisfied: future>=0.17.1 in /usr/local/lib/python3.8/dist-packages (from neptune-client) (0.18.2)\n",
            "Requirement already satisfied: requests-oauthlib>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from neptune-client) (1.3.1)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.8/dist-packages (from neptune-client) (1.15.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from neptune-client) (21.3)\n",
            "Requirement already satisfied: swagger-spec-validator>=2.7.4 in /usr/local/lib/python3.8/dist-packages (from neptune-client) (3.0.3)\n",
            "Requirement already satisfied: oauthlib>=2.1.0 in /usr/local/lib/python3.8/dist-packages (from neptune-client) (3.2.2)\n",
            "Requirement already satisfied: websocket-client!=1.0.0,>=0.35.0 in /usr/local/lib/python3.8/dist-packages (from neptune-client) (1.4.2)\n",
            "Requirement already satisfied: GitPython>=2.0.8 in /usr/local/lib/python3.8/dist-packages (from neptune-client) (3.1.30)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.8/dist-packages (from neptune-client) (1.26.13)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from neptune-client) (1.3.5)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.8/dist-packages (from neptune-client) (7.1.2)\n",
            "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from boto3>=1.16.0->neptune-client) (0.6.0)\n",
            "Requirement already satisfied: botocore<1.30.0,>=1.29.45 in /usr/local/lib/python3.8/dist-packages (from boto3>=1.16.0->neptune-client) (1.29.45)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.8/dist-packages (from boto3>=1.16.0->neptune-client) (1.0.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from bravado<12.0.0,>=11.0.0->neptune-client) (4.4.0)\n",
            "Requirement already satisfied: monotonic in /usr/local/lib/python3.8/dist-packages (from bravado<12.0.0,>=11.0.0->neptune-client) (1.6)\n",
            "Requirement already satisfied: simplejson in /usr/local/lib/python3.8/dist-packages (from bravado<12.0.0,>=11.0.0->neptune-client) (3.18.1)\n",
            "Requirement already satisfied: bravado-core>=5.16.1 in /usr/local/lib/python3.8/dist-packages (from bravado<12.0.0,>=11.0.0->neptune-client) (5.17.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.8/dist-packages (from bravado<12.0.0,>=11.0.0->neptune-client) (6.0)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.8/dist-packages (from bravado<12.0.0,>=11.0.0->neptune-client) (1.0.4)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.8/dist-packages (from bravado<12.0.0,>=11.0.0->neptune-client) (2.8.2)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.8/dist-packages (from GitPython>=2.0.8->neptune-client) (4.0.10)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.20.0->neptune-client) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.20.0->neptune-client) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.20.0->neptune-client) (2022.12.7)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.8/dist-packages (from swagger-spec-validator>=2.7.4->neptune-client) (4.3.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->neptune-client) (3.0.9)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->neptune-client) (2022.7)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.8/dist-packages (from pandas->neptune-client) (1.21.6)\n",
            "Requirement already satisfied: jsonref in /usr/local/lib/python3.8/dist-packages (from bravado-core>=5.16.1->bravado<12.0.0,>=11.0.0->neptune-client) (1.0.1)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.8/dist-packages (from gitdb<5,>=4.0.1->GitPython>=2.0.8->neptune-client) (5.0.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (22.2.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (0.19.2)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (5.10.1)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.8/dist-packages (from importlib-resources>=1.4.0->jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (3.11.0)\n",
            "Requirement already satisfied: isoduration in /usr/local/lib/python3.8/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (20.11.0)\n",
            "Requirement already satisfied: jsonpointer>1.13 in /usr/local/lib/python3.8/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (2.3)\n",
            "Requirement already satisfied: rfc3339-validator in /usr/local/lib/python3.8/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (0.1.4)\n",
            "Requirement already satisfied: uri-template in /usr/local/lib/python3.8/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (1.2.0)\n",
            "Requirement already satisfied: fqdn in /usr/local/lib/python3.8/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (1.5.1)\n",
            "Requirement already satisfied: webcolors>=1.11 in /usr/local/lib/python3.8/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (1.12)\n",
            "Requirement already satisfied: rfc3987 in /usr/local/lib/python3.8/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (1.3.8)\n",
            "Requirement already satisfied: arrow>=0.15.0 in /usr/local/lib/python3.8/dist-packages (from isoduration->jsonschema->swagger-spec-validator>=2.7.4->neptune-client) (1.2.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a resource originally about Bayesian Neural Networks,  by TensorChiefs found as supplemental coding material for their book [Probabilistic Deep Learning](https://tensorchiefs.github.io/dl_book/), their original code is [here](https://colab.research.google.com/drive/1ZCNuGd9z_ZvJ7irFH-FQmqlFXP5jeK8z#scrollTo=TNFhkloWTImi). **I  added  Test Time Augmentation.**\n",
        "\n",
        "Future: I plan to add [time and memory run profiles](https://colab.research.google.com/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/01.07-Timing-and-Profiling.ipynb#scrollTo=HdeOK7Qhw0wp) to also compare the methods"
      ],
      "metadata": {
        "id": "Gt-QCpnVHyFd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Just do TTA experiments? { display-mode: \"form\" }\n",
        "#@markdown This Colab will skip everything but TTA if checked\n",
        "just_TTA  = True #@param {type: \"boolean\"} \n",
        "\n",
        "if just_TTA:\n",
        "  print(\"Will skip everything but TTA experiments\")"
      ],
      "metadata": {
        "id": "9TE-7aBxoXLI",
        "outputId": "df473bbf-7305-4264-b275-6c6bf51663d7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Will skip everything but TTA experiments\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Goal:** In this notebook you will investigate which advantages Bayesian NNs can offer in a classification task. You will use train data from 9 of the 10 classes in the Cifar10 dataset to fit different three probabilistic NN.\n",
        "First you fit a \"traditional\" non-Bayesian NN and then you will fit two Bayesian NN, one via variational inference and one via dropout. You will compare the accuracy of the different NN on the 9 known classes. Further you will investigate and compare the uncertainties expressed by the NNs for both the known classes and unknown class. Finally you will use these uncertainties to detect novel classes and filter out uncertain predictions.\n",
        "\n",
        "**Usage:** The idea of the notebook is that you try to understand the provided code by running it, checking the output and playing with it by slightly changing the code and rerunning it.\n",
        "\n",
        "**Dataset:** You work with the Cifar10 dataset. You have 60'000 32x32 pixel color images of 10 classes (\"airplane\",\"automobile\",\"bird\",\"cat\",\"deer\",\"dog\",\"frog\",\"horse\",\"ship\",\"truck\"). You will delete all images from the class \"horse\" in the training dataset to simulate a novel class. Note that we keep the class \"horse\" in the test dataset.\n",
        "\n",
        "**Content:**\n",
        "* Load the Cifar10 dataset\n",
        "* Delete all images of the class \"horse\" from the training dataset\n",
        "* Split the train dataset into a train and validation dataset (60:40 split) \n",
        "* Fit a non-Bayesian NN \n",
        "* Fit a Bayesian NN via variational inference \n",
        "* Fit a Bayesian NN via dropout \n",
        "* Compare the  of the accuracy of the models on the known classes\n",
        "* Compare the  of the uncertainties of the models on the known and unknown classes\n",
        "* Use the uncertainties to filter uncertain predictions\n",
        "\n",
        "| [open in colab](https://colab.research.google.com/github/tensorchiefs/dl_book/blob/master/chapter_08/nb_ch08_04.ipynb)"
      ],
      "metadata": {
        "id": "NwDpm0Fa1HuH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JwdOrJ_JMC4W",
        "outputId": "98c92812-e812-4c35-f5f8-40e21d2f0a57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab only includes TensorFlow 2.x; %tensorflow_version has no effect.\n"
          ]
        }
      ],
      "source": [
        "try: #If running in colab \n",
        "    import google.colab\n",
        "    IN_COLAB = True \n",
        "    %tensorflow_version 2.x\n",
        "except:\n",
        "    IN_COLAB = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KdtMWyE3fYI3"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "if (not tf.__version__.startswith('2')): #Checking if tf 2.0 is installed\n",
        "    print('Please install tensorflow 2.0 to run this notebook')\n",
        "print('Tensorflow version: ',tf.__version__, ' running in colab?: ', IN_COLAB)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BLerHJCHFWMy"
      },
      "outputs": [],
      "source": [
        "# I commented this out because I think there are too many outdated dependencies\n",
        "\n",
        "# if IN_COLAB:\n",
        "#     !pip install tensorflow_probability==0.8.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rE8KfEvxFxq9"
      },
      "source": [
        "#### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ESl26w7xML1z"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "import urllib.request\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#I added this below from StackOverflow to see if it wouldsolve import issues\n",
        "#https://stackoverflow.com/questions/62743492/cannot-import-tensorflow-probability\n",
        "# !pip install --upgrade tf_agents\n",
        "\n",
        "import tensorflow_probability as tfp\n",
        "\n",
        "%matplotlib inline\n",
        "plt.style.use('default')\n",
        "\n",
        "print(\"TFP Version\", tfp.__version__)\n",
        "print(\"TF  Version\",tf.__version__)\n",
        "\n",
        "## Adding Test time Augmentation TTA\n",
        "import keras\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.utils import to_categorical\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import datetime, time\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "begin = datetime.datetime.now()\n",
        "\n",
        "# now.strftime('%Y-%m-%d %H:%M:%S')\n",
        "\n",
        "print(begin)\n"
      ],
      "metadata": {
        "id": "M8jhznGioGg3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmemfmRxF0pt"
      },
      "source": [
        "In the next cell you disable the tensorflow eager mode. We need to do this because otherwise we would get errors for the variational inference NN and we  would not be able to turn on and off the dropout in the MC dropout NN."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XHzeQBbhcV3W"
      },
      "outputs": [],
      "source": [
        "if just_TTA:\n",
        "  print(\"Not disabling eager execution mode because just doing TTA\")\n",
        "else:\n",
        "    from tensorflow.python.framework.ops import disable_eager_execution\n",
        "    disable_eager_execution()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APxoLqsjkM31"
      },
      "source": [
        "#### Loading and preparation of the dataset\n",
        "\n",
        "Let's load the cifar 10 dataset. It is already splited into a train and test dataset. To get a feeling for the dataset, you plot a random example of each class of the trainingset. You can see that the images are quite small and its not always easy to see the class on the image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RHnWbEuDMzKv"
      },
      "outputs": [],
      "source": [
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bAqFwLxMPHQU"
      },
      "outputs": [],
      "source": [
        "labels=np.array([\"airplane\",\"automobile\",\"bird\",\"cat\",\"deer\",\"dog\",\"frog\",\"horse\",\"ship\",\"truck\"])\n",
        "#sample image of each label\n",
        "np.random.seed(22)\n",
        "plt.figure(figsize=(20,20))\n",
        "for i in range(0,len(np.unique(y_train))):\n",
        "    rmd=np.random.choice(np.where(y_train==i)[0],1)\n",
        "    plt.subplot(1,10,i+1)\n",
        "    img=x_train[rmd]\n",
        "    plt.imshow(img[0,:,:,:])\n",
        "    plt.title(np.str(y_train[rmd][0][0])+ \": \" +labels[i],fontsize=16)\n",
        "plt.savefig(\"Figure_8_cifar.pdf\")\n",
        "#from google.colab import files\n",
        "#files.download('Figure_8_cifar.pdf') \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQZdM-PHG2x2"
      },
      "source": [
        "To simualte a novel class, you will delete all images of the class \"horse\" from the traning dataset. Note that you do this only in the trainset, the test dataset stays the same. In the train dataset you will now have 9 classes with 5'000 images of every class, in total 45'000 images. This will be your traning dataset for the three models.   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Go4tZ3Xwf8Wc"
      },
      "outputs": [],
      "source": [
        "y_train_no_horse=np.delete(y_train,np.where(y_train==7)[0])\n",
        "x_train_no_horse=np.delete(x_train,np.where(y_train==7)[0],axis=0)\n",
        "print(y_train_no_horse.shape)\n",
        "print(x_train_no_horse.shape)\n",
        "y_train_no_horse=np.array(pd.get_dummies(y_train_no_horse))\n",
        "labels_no_horse=np.array([\"airplane\",\"automobile\",\"bird\",\"cat\",\"deer\",\"dog\",\"frog\",\"ship\",\"truck\"])\n",
        "print(y_train_no_horse.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymY2h4GzLrQe"
      },
      "source": [
        "Now you split the new training dataset without the horses randomly into a train and validationset. You use a 60:40 ratio for the split, so you have 27'000 train images with 9 classes and 18'000 validation images with 9 classes. The test dataset has 10'000 images and 10 classes. Note that we have a novel (unknown) class \"horse\" in the testset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pfTBvgg8SkiZ"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_val, y_train, y_val = train_test_split(x_train_no_horse, y_train_no_horse, test_size=0.4, random_state=22)\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)\n",
        "print(x_val.shape)\n",
        "print(y_val.shape)\n",
        "print(x_test.shape)\n",
        "print(y_test.shape)\n",
        "\n",
        "#Before training the NNs with the data, we normalize the data to be in the range between -1 and 1. \n",
        "x_train=((x_train/255)-0.5)*2\n",
        "x_val=((x_val/255)-0.5)*2\n",
        "x_test=((x_test/255)-0.5)*2\n",
        "\n",
        "x_test.min(), x_test.max()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# #Experiment to bring it back to 0 and 255\n",
        "# x_test_exp=((x_test/2)+0.5)*255\n",
        "# x_test_exp.min(), x_test_exp.max()\n",
        "\n",
        "# Experiment with experument to bring it to 0 and 1.\n",
        "def revertscale_fn(img):\n",
        "    img = img.astype(np.float32) / 2\n",
        "    img = (img + 0.5) * 255\n",
        "    return img\n",
        "# gen = ImageDataGenerator(\n",
        "#     preprocessing_function=revertscale_fn,\n",
        "#     rescale = 1./255,\n",
        "# )"
      ],
      "metadata": {
        "id": "uLC0VoMWr0el"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVA0YHjvYlB_"
      },
      "source": [
        "## Non-Bayesian CNN\n",
        "\n",
        "In this section we use a CNN with two convolutional blocks, followed by maxpooling layers. You use 8 kernels with the size 3x3 in the first convolutional block and in the second block you use 16 kernels with the size 3x3. The maxpoolingsize is 2x2 pixels. After the feature extraction you use a flatten layer and do the classification with 3 fully connected layers. Because the training takes a lot of time, you will load an already trained CNN with the learning curve."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XLOW7UPyYkKw"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Convolution2D,MaxPooling2D,Dropout,Flatten,Dense\n",
        "from tensorflow.keras import Sequential\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Convolution2D(8,kernel_size=(3,3),padding=\"same\", activation = 'relu',input_shape=(32,32,3)))\n",
        "model.add(Convolution2D(8,kernel_size=(3,3),padding=\"same\", activation = 'relu'))\n",
        "model.add(MaxPooling2D((2,2)))\n",
        "model.add(Convolution2D(16,kernel_size=(3,3),padding=\"same\", activation = 'relu'))\n",
        "model.add(Convolution2D(16,kernel_size=(3,3),padding=\"same\", activation = 'relu'))\n",
        "model.add(MaxPooling2D((2,2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(100, activation = 'relu'))\n",
        "model.add(Dropout((0.5)))\n",
        "model.add(Dense(100, activation = 'relu'))\n",
        "model.add(Dropout((0.5)))\n",
        "model.add(Dense(9, activation = 'softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer=\"adam\", metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PMB1B7hjYkFg"
      },
      "outputs": [],
      "source": [
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/tensorchiefs/dl_book/master/data/history_baseline_cifar10.csv\", \"history_baseline_cifar10.csv\")\n",
        "history=np.loadtxt(\"history_baseline_cifar10.csv\",delimiter=\",\")\n",
        "\n",
        "plt.figure(figsize=(12,5))\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(history[:,0])\n",
        "plt.plot(history[:,1])\n",
        "plt.ylim(0, 1)\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'valid'], loc='lower right')\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(history[:,2])\n",
        "plt.plot(history[:,3])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'valid'], loc='upper right')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sqwqWP8R4_6Y"
      },
      "outputs": [],
      "source": [
        "# load trained weights of the model\n",
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/tensorchiefs/dl_book/master/data/model_cifar10_weights.hdf5\", \"model_cifar10_weights.hdf5\")\n",
        "model.load_weights(\"model_cifar10_weights.hdf5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I didn't know if the model was trained using EarlyStop or Val_Loss, I just see that the previous model was trained on accuracy so I want to train the model and double check that it was stopped around epoch 20 and not epoch 50 where the validation loss increased.  (ETA - Verified it was epoch 20)\n",
        "\n"
      ],
      "metadata": {
        "id": "ftsRMkDmteds"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# callback = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=25)\n",
        "# history = model.fit(x=x_train, y=y_train, batch_size=64, epochs=50, verbose='0', callbacks = [callback], validation_data=(x_val, y_val))\n",
        "# history = history.history"
      ],
      "metadata": {
        "id": "iUmcvUj0tu2D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# First of all call dict. items() to return a group of the key-value pairs in the dictionary.\n",
        "# Then use list(obj) with this group as an object to convert it to a list.\n",
        "# At last, call numpy. array(data) with this list as data to convert it to an array.\n",
        " "
      ],
      "metadata": {
        "id": "albebD__9ciU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plt.figure(figsize=(12,5))\n",
        "# plt.subplot(1,2,1)\n",
        "# plt.plot(history['accuracy'])\n",
        "# plt.plot(history['val_accuracy'])\n",
        "# plt.ylim(0, 1)\n",
        "# plt.title('model accuracy')\n",
        "# plt.ylabel('accuracy')\n",
        "# plt.xlabel('epoch')\n",
        "# plt.legend(['train', 'valid'], loc='lower right')\n",
        "# plt.subplot(1,2,2)\n",
        "# plt.plot(history['loss'])\n",
        "# plt.plot(history['val_loss'])\n",
        "# plt.title('model loss')\n",
        "# plt.ylabel('loss')\n",
        "# plt.xlabel('epoch')\n",
        "# plt.legend(['train', 'valid'], loc='upper right')\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "_MR9m7SuvBPJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9fsQMnMpRgz"
      },
      "source": [
        "Here you save the indices of the known and the unknown (horse) classes. You will use them later to evaluate the uncertainty measures."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SvNbeGEtKcNb"
      },
      "outputs": [],
      "source": [
        "known_idx=np.where(y_test!=7)[0]\n",
        "unknown_idx=np.where(y_test==7)[0]\n",
        "\n",
        "print(len(known_idx))\n",
        "print(len(unknown_idx))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "what is the given acuracy on the train and **val** set?"
      ],
      "metadata": {
        "id": "IVReQ86HyWb3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pred=model.predict(x_val)  #array (18000, 9)\n",
        "# pred_max_p=np.max(pred,axis=1) #array of (18000,)\n",
        "\n",
        "# entropy=np.array([-np.sum( pred[i] * np.log2(pred[i] + 1E-14)) for i in range(0,len(pred))])\n",
        "# nll_=-np.log(pred_max_p)\n",
        "# pred_labels=np.array([labels_no_horse[np.argmax(pred[i])] for i in range(0,len(pred))])\n",
        "# true_labels=np.array([labels_no_horse[np.argmax(y_val[i])] for i in range(0,len(y_val))])\n",
        "# val_cm = confusion_matrix(true_labels, pred_labels)\n",
        "\n",
        "# disp = ConfusionMatrixDisplay(confusion_matrix=val_cm, display_labels=labels_no_horse)\n",
        "\n",
        "# disp.plot(cmap=plt.cm.Blues)\n",
        "# plt.show()\n",
        "\n",
        "\n",
        "# val_acc_all=np.average(true_labels==pred_labels)\n",
        "# val_acc_known=np.average(true_labels[known_idx]==pred_labels[known_idx])\n",
        "# val_acc_all, val_acc_known\n"
      ],
      "metadata": {
        "id": "kHWYIGcIyaDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2-9VwElMO_Lo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBqfSpLWhNNH"
      },
      "source": [
        "## Accuracy on the the known labels in the train set for all three models \n",
        "In this section you will calculate the accuracies and of all three models. For the non bayesian NN, you will predict every test image once and for the two bayesian NN, you will predict every image 50 times and then takes the mean of all predicted classes. \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Ja_1L-xp-9u"
      },
      "source": [
        "#### Non-Bayesian prediction\n",
        "\n",
        "Here you predict the labels for the non-bayesian CNN and calculate the uncertainty measures. You calculate the nll and the entropy, note that there is no total standart deviation in the non-bayesian model, because the same image will always get the same prediction. This is also the reason why we don't need to predict the same image for multiple times."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This is how you get accuracy from the Baseline Vanilla CNN. These predictions only work for deterministic sources\n",
        "\n",
        "# pred=model.predict(x_test)\n",
        "# pred_max_p=np.max(pred,axis=1)\n",
        "# entropy=np.array([-np.sum( pred[i] * np.log2(pred[i] + 1E-14)) for i in range(0,len(pred))])\n",
        "# nll=-np.log(pred_max_p)\n",
        "# pred_labels=np.array([labels_no_horse[np.argmax(pred[i])] for i in range(0,len(pred))])\n",
        "# true_labels=np.array([labels[y_test[i][0]] for i in range(0,len(y_test))])\n",
        "\n",
        "# test_acc_all=np.average(true_labels==pred_labels)\n",
        "# test_acc_known=np.average(true_labels[known_idx]==pred_labels[known_idx])\n",
        "# test_acc_all, test_acc_known"
      ],
      "metadata": {
        "id": "oml843PryqkZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K-bOyOue0_hj"
      },
      "outputs": [],
      "source": [
        "pred=model.predict(x_test)\n",
        "pred_max_p=np.max(pred,axis=1)\n",
        "entropy=np.array([-np.sum( pred[i] * np.log2(pred[i] + 1E-14)) for i in range(0,len(pred))])\n",
        "nll=-np.log(pred_max_p)\n",
        "pred_labels=np.array([labels_no_horse[np.argmax(pred[i])] for i in range(0,len(pred))])\n",
        "true_labels=np.array([labels[y_test[i][0]] for i in range(0,len(y_test))])\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Confusion Matrix for Test-Data from Vanilla-CNN Model (no TTA)"
      ],
      "metadata": {
        "id": "3tCUMPDoAmHc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " \n",
        "cm = confusion_matrix(true_labels, pred_labels)\n",
        "\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
        "\n",
        "disp.plot(cmap=plt.cm.Blues)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "oo2lBt-O-851"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R98YlXwe0_bS"
      },
      "outputs": [],
      "source": [
        "\n",
        "test_acc_all=np.average(true_labels==pred_labels)\n",
        "test_acc_known=np.average(true_labels[known_idx]==pred_labels[known_idx])\n",
        "test_acc_all, test_acc_known"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The non-bayesian CNN scores 58.45% accuracy on all 10 items, because it misidentified horse (it can't possibly classify it correctly), but it scored 64.9% accuracy on the remaining categories."
      ],
      "metadata": {
        "id": "sWHBMHOyu1bQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New"
      ],
      "metadata": {
        "id": "nHGYBSS1PHkB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I want to restart my runs here, so I don't have to reload everything (data, history of model training, imports) so what I need to do is Delete Variables created from the previous run."
      ],
      "metadata": {
        "id": "-lkU5FkfPS2X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def delete_old_params():\n",
        "  del(tta_steps, bs,  preds, predictions_tta, cm_tta,  entropy_tta, nll_tta, pred_labels_tta, df,\n",
        "      test_acc_all_tta, final_pred, pred_tta, pred_max_p_tta, pred_std_tta, pred_tta_mean_max_p)\n",
        "  del(predictions_tta_idx, final_pred_idx)\n",
        "  del(disp_tta)\n",
        "  del(X_batch, y_batch)\n",
        "  del(test_datagen)\n",
        "\n",
        "# try: \n",
        "#   list_of_vars =  [params, run,  test_datagen, tta_steps, bs, display_datagen, preds, predictions_tta, cm_tta,  entropy_tta, nll_tta, pred_labels_tta, df,\n",
        "#       test_acc_all_tta, final_pred, pred_tta, pred_max_p_tta, pred_std_tta, pred_tta_mean_max_p,\n",
        "#   predictions_tta_idx, final_pred_idx, disp_tta, X_batch, y_batch]\n",
        "# except NameError:\n",
        "#   list_of_vars =  [params, run,  test_datagen, tta_steps, bs, display_datagen, preds, predictions_tta, cm_tta,  entropy_tta, nll_tta, pred_labels_tta, df,\n",
        "#       test_acc_all_tta, final_pred, pred_tta, pred_max_p_tta, pred_std_tta, pred_tta_mean_max_p,\n",
        "#   predictions_tta_idx, final_pred_idx, disp_tta, X_batch, y_batch]\n",
        "\n",
        "# for i in list_of_vars:\n",
        "#   try:  \n",
        "#     i\n",
        "#   except NameError:\n",
        "#    del(i)\n"
      ],
      "metadata": {
        "id": "zkf6k9goPSCP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start = datetime.datetime.now()\n",
        "print(start)"
      ],
      "metadata": {
        "id": "dwF8_ZiEXjsY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ðŸ†• Test Time Augmentation\n",
        "\n",
        "Here I predict the labels for the traditional CNN with Test Time Augmentation and possibly calculate the uncertainty measures.  I predict the same image for 50 times and calculate the mean predicted probabilities, the nll, the entropy, and total standard deviations. "
      ],
      "metadata": {
        "id": "ErVsj1dAKSXs"
      }
    },
    {
      "metadata": {
        "id": "_ICzIOrPQ6Lu"
      },
      "cell_type": "markdown",
      "source": [
        "Define the Test Time Image Generator\n",
        "\n",
        "Keras has predefined choices of [augmentations defined here](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator) and explained visually well [here](https://towardsdatascience.com/exploring-image-data-augmentation-with-keras-and-tensorflow-a8162d89b844) in a TowardsDataScience Medium Blog Post "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import neptune.new as neptune\n",
        "from neptune.new.types import File\n"
      ],
      "metadata": {
        "id": "UNYO-XeiL2I8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### def new_run()"
      ],
      "metadata": {
        "id": "7mHjJxjTUKZf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize a New Neptune Run function - takes no input yet? #returns a run\n",
        "def new_run():\n",
        "    run = neptune.init(\n",
        "      project=\"rachel.ramirez2006/RotationsOnly\",\n",
        "      api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiI4NWZjMGEyYy1mZGI3LTQ1NWMtOGUyZi0wODc2OWI2OGU3NzcifQ==\",)  # your credentials\n",
        "    return run\n"
      ],
      "metadata": {
        "id": "_lSZknYQL38h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9zJGPFtEQ6Lw"
      },
      "cell_type": "code",
      "source": [
        "tta_steps = 50\n",
        "bs = 64\n",
        "# epochs = 15"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are the original tried test_datagen that have resulted in the highest accuracy scores:\n",
        "\n",
        "        tta_steps = 50,\n",
        "        bs = 64,\n",
        "        \n",
        "        shear_range=0.1,\n",
        "        zoom_range=0.1,\n",
        "        horizontal_flip=True,\n",
        "        rotation_range=10.,\n",
        "        fill_mode='reflect', \n",
        "        width_shift_range = 0.1, \n",
        "        height_shift_range = 0.1)"
      ],
      "metadata": {
        "id": "QOEgxOVed9sE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### def make_init_params(tta_steps=50, bs=64, low_rotation=0, high_rotation=10,  fill_mode = 'reflect'):"
      ],
      "metadata": {
        "id": "DeTCvqUQUQVN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_init_params(tta_steps=50, bs=64, low_rotation=0, high_rotation=10,  fill_mode = 'reflect'):\n",
        "  params = {\n",
        "        'tta_steps': tta_steps,\n",
        "        'bs': bs,\n",
        "         'low_rotation': low_rotation,\n",
        "         'high_rotation': high_rotation,  \n",
        "          # 'rotation_range': 5, \n",
        "          # 'shear_range': 5,\n",
        "          # 'zoom_range' : 0.1,\n",
        "          # 'horizontal_flip': True, \n",
        "          # 'width_shift_range' : 0.1,\n",
        "          # 'height_shift_range' : 0.1 ,\n",
        "\n",
        "          # 'brightness_range' : (0.9,1.1),\n",
        "          # 'channel_shift_range': 45/255,\n",
        "          'fill_mode' : fill_mode,\n",
        "          # 'featurewise_center': False,\n",
        "          # 'samplewise_center': False,\n",
        "          # 'featurewise_std_normalization': False,\n",
        "          # 'samplewise_std_normalization' : False,\n",
        "          # 'zca_whitening' : False,\n",
        "\n",
        "\n",
        "            # 'constant': kkkkkkkk|abcd|kkkkkkkk (cval=k)\n",
        "            # 'nearest': aaaaaaaa|abcd|dddddddd\n",
        "            # 'reflect': abcddcba|abcd|dcbaabcd\n",
        "            # 'wrap': abcdabcd|abcd|abcdabcd\n",
        "            # cval\tFloat or Int. Value used for points outside the boundaries when fill_mode = \"constant\".\n",
        "        # 'vertical_flip': False\n",
        "        }\n",
        "  return params\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "EOtuWzRnMFkq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### def make_image_data_generator():\n"
      ],
      "metadata": {
        "id": "bA9k7Q7mUgOR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_image_data_generator(params):\n",
        "  #takes a dictionary of parameters \n",
        "  #makes display_datagen to display augmentations on a batch\n",
        "  #saves display to run\n",
        "  #returns test_datagen\n",
        "\n",
        "  test_datagen = ImageDataGenerator(\n",
        "          #preprocessing_function=revertscale_fn,   #hoping this will increase the overall accuracy of TTA from 0.448 to something more similar to the vanilla CNN without TTA - IT DID NOT\n",
        "          #rescale = 1./255,\n",
        "          # shear_range=params['shear_range'],\n",
        "          # zoom_range=params['zoom_range'],\n",
        "          # horizontal_flip=params['horizontal_flip'],\n",
        "          rotation_range= params['high_rotation'],\n",
        "          fill_mode=params['fill_mode'] ,\n",
        "        #   width_shift_range = params['width_shift_range'], \n",
        "        #   height_shift_range = params['height_shift_range'],\n",
        "\n",
        "        #  featurewise_center = params['featurewise_center'],\n",
        "        #  samplewise_center  = params['samplewise_center'],\n",
        "        #  featurewise_std_normalization = params['featurewise_std_normalization'],\n",
        "        #  samplewise_std_normalization = params['samplewise_std_normalization'],\n",
        "\n",
        "        #   zca_whitening = params['zca_whitening'],\n",
        "\n",
        "        #   brightness_range = params['brightness_range'],\n",
        "  \n",
        "          # channel_shift_range = params['channel_shift_range'],\n",
        "          # vertical_flip = params['vertical_flip']\n",
        "          )\n",
        "\n",
        "  display_datagen = ImageDataGenerator(\n",
        "          preprocessing_function=revertscale_fn,   #yay that worked\n",
        "          rescale = 1./255,\n",
        "            # shear_range=params['shear_range'],\n",
        "            # zoom_range=params['zoom_range'],\n",
        "            # horizontal_flip=params['horizontal_flip'],\n",
        "            rotation_range= params['high_rotation'],\n",
        "            fill_mode=params['fill_mode'] ,\n",
        "          #   width_shift_range = params['width_shift_range'], \n",
        "          #   height_shift_range = params['height_shift_range'],\n",
        "\n",
        "          #  featurewise_center = params['featurewise_center'],\n",
        "          #  samplewise_center  = params['samplewise_center'],\n",
        "          #  featurewise_std_normalization = params['featurewise_std_normalization'],\n",
        "          #  samplewise_std_normalization = params['samplewise_std_normalization'],\n",
        "\n",
        "          #   zca_whitening = params['zca_whitening'],\n",
        "\n",
        "          #   brightness_range = params['brightness_range'],\n",
        "    \n",
        "          #   channel_shift_range = params['channel_shift_range'],\n",
        "          #   vertical_flip = params['vertical_flip'],\n",
        "    )\n",
        "\n",
        "  display_datagen.fit(x_train)\n",
        "\n",
        "  tta_plt = plt.figure(figsize=(5,3)) #I want to name this plot so I can save it in Neptune.\n",
        "\n",
        "  for X_batch, y_batch in display_datagen.flow(x_train, y_train, batch_size=15):\n",
        "      for i in range(0, 15):\n",
        "          plt.subplot(3, 5, i+1)\n",
        "          plt.imshow(X_batch[i].reshape(32, 32, 3), cmap=plt.get_cmap('gray'))\n",
        "          plt.axis('off')\n",
        "      plt.tight_layout(pad=0.1)\n",
        "      plt.show()\n",
        "      break\n",
        "\n",
        "  run[\"tta_image_preview\"].upload(tta_plt)   #I want to save this plot in Neptune.\n",
        "  # run[\"tta_image_preview-interactive\"].upload(File.as_html(tta_plt)) #doesnt work\n",
        "\n",
        "  return test_datagen\n"
      ],
      "metadata": {
        "id": "seVCCesQUZLx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### def make_predictions_with_tta():\n"
      ],
      "metadata": {
        "id": "Cuo55Oa-Un49"
      }
    },
    {
      "metadata": {
        "id": "ZVU-2IQQQ6Lz"
      },
      "cell_type": "code",
      "source": [
        "def make_predictions_with_tta():\n",
        "  #takes test_datagen, model, and x_test, tta_steps\n",
        "  #returns final_pred the mean of all tta_steps\n",
        "  # predictions_tta_array = np.empty((10000,9))\n",
        "  predictions_tta = []\n",
        "  #predictions_tta_df =  pd.DataFrame([])\n",
        "  test_datagen.fit(x_train)\n",
        "\n",
        "  #for i in tqdm(range(tta_steps)):\n",
        "  for i in tqdm(range(5)):\n",
        "      preds = model.predict(test_datagen.flow(x_test, batch_size=bs, shuffle=False), steps = len(x_test)/bs, verbose=0)   #Changed X_val to X_test \n",
        "      # print(\"Iteration i: \", i, \" Preds Shape is \", preds.shape)\n",
        "      # preds.resize((1, 10000, 9))\n",
        "      predictions_tta.append(preds)\n",
        "  \n",
        "  final_pred = np.mean(predictions_tta, axis=0)\n",
        " \n",
        "\n",
        "  return final_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# predictions_tta[0][0]   #the first guess of #tta_steps for the 0th image, 1st prob guess is \"3\" followed by \"5\"\n",
        "# predictions_tta[9][0]   #the 9th   guess of #tta_steps for the 0th image, 1st largest prob is \"3\" followed by \"5\"\n",
        "# y_test[0]               #the actual label of the 0th test image is \"3\" so it would have got this correct."
      ],
      "metadata": {
        "id": "MOMekun2vugU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Size of things\n",
        "#final_pred         # An array of 10000x9 10000 images with 9 classes of probabilities\n",
        "#predictions_tta    # A list of TTA_#_OF_Steps of  10000 images with 9 classes of probabilities"
      ],
      "metadata": {
        "id": "mqafCtBM5Awi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### def calc_metrics()"
      ],
      "metadata": {
        "id": "MAyI_vRrUsho"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# final_pred\n",
        "def calc_metrics():\n",
        "  #takes x_test predictions and truth and calculates metrics like max p, std def, entropy\n",
        "  # returns run information for Neptune\n",
        "  pred_tta=np.zeros((len(x_test),9))\n",
        "  pred_max_p_tta=np.zeros((len(x_test)))\n",
        "  pred_std_tta=np.zeros((len(x_test)))\n",
        "  entropy_tta = np.zeros((len(x_test)))\n",
        "\n",
        "\n",
        "  for i in tqdm(range(0,len(x_test))):\n",
        "    #multi_img=np.tile(x_test[i],(50,1,1,1))\n",
        "    #preds=model_mc_pred([multi_img,1])\n",
        "    pred_tta[i]= final_pred[i]\n",
        "\n",
        "    pred_max_p_tta[i]=np.argmax(final_pred[i]) #mean over n runs of every proba class\n",
        "    pred_std_tta[i]= np.sqrt(np.sum(np.var(final_pred[i])))\n",
        "    entropy_tta[i] = -np.sum( pred_tta[i] * np.log2(pred_tta[i] + 1E-14)) #Numerical Stability\n",
        "\n",
        "\n",
        "  # print(\"Image 1 Pred TTA  is \")\n",
        "  # print(pred_tta[1])\n",
        "  # print(\"Image 1 Pred Max P TTA  is \")\n",
        "  # print(pred_max_p_tta[1])\n",
        "  # print(\"Image 1 Entropy is \")\n",
        "  # print(entropy_tta[1])\n",
        "\n",
        "    \n",
        "\n",
        "  pred_labels_tta=np.array([labels_no_horse[np.argmax(pred_tta[i])] for i in range(0,len(pred_tta))])\n",
        "  pred_tta_mean_max_p=np.array([pred_tta[i][np.argmax(pred_tta[i])] for i in range(0,len(pred_tta))])\n",
        "  nll_tta=-np.log(pred_tta_mean_max_p)\n",
        "\n",
        "\n",
        "  # print(\"Image 1's Pred Labels TTA is \")\n",
        "  # print(pred_labels_tta[1]) \n",
        "  # print(\"Image 1's Pred_TTA_MEAN_MAX_P is \")\n",
        "  # print(pred_tta_mean_max_p[1])\n",
        "  # print(\"Image 1's NLL is  \")\n",
        "  # print(nll_tta[1])\n",
        "\n",
        "\n",
        "  test_acc_all_tta=np.average(true_labels==pred_labels_tta)\n",
        "  test_acc_known_tta=np.average(true_labels[known_idx]==pred_labels_tta[known_idx])\n",
        "  print(test_acc_all_tta, test_acc_known_tta)\n",
        "  run[\"test_acc_all_tta\"] = test_acc_all_tta\n",
        "  run[\"test_acc_known_tta\"] = test_acc_known_tta\n",
        "  run[\"pred_std_tta\"] = pred_std_tta\n",
        "  run[\"pred_labels_tta\"] = pred_labels_tta\n",
        "  run[\"nll_tta\"] = nll_tta\n",
        "  # run[\"nll_tta_log\"].log(nll_tta)  #\"value of type numpy.ndarray is not supported please use file.as_image()\"\n",
        "\n",
        "  run[\"entropy_tta\"] = entropy_tta\n",
        "  run[\"pred_max_p_tta\"] = pred_max_p_tta\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  cm_tta = confusion_matrix(true_labels, pred_labels_tta)\n",
        "  disp_tta = ConfusionMatrixDisplay(confusion_matrix=cm_tta, display_labels=labels)\n",
        "  disp_tta.plot(cmap=plt.cm.Blues)\n",
        "  # plt.show()\n",
        "  plt.savefig(\"cm_tta.png\") #save as png\n",
        "  run[\"test_cm_tta\"] = cm_tta\n",
        "  run[\"test_cm_tta_pic\"].upload(\"cm_tta.png\")\n",
        "  \n",
        "  # run['predictions_tta_dataframe'] = df\n",
        "\n",
        " \n",
        "  df = pd.DataFrame(data={'y_test': y_test.reshape(-1), 'y_pred': final_pred.argmax(axis=1), 'y_pred_probability': final_pred.max(axis=1), 'entropy': np.round(entropy_tta, 3) })\n",
        "\n",
        "\n",
        "  df = df.assign(Test_Labels = lambda df: labels[df.y_test])\n",
        "  df = df.assign(Pred_Labels = lambda df: labels_no_horse[df.y_pred])\n",
        "  df = df.assign(Result=lambda df: df.Test_Labels==df.Pred_Labels)\n",
        "  df['Result'].value_counts()\n",
        "  correct_idx = df[df[\"Result\"]==True].index.values\n",
        "\n",
        "  # ---\n",
        "  #I want to save the Index values of the Correctly Classified images\n",
        "\n",
        "  df[\"Result\"==True].index\n",
        "\n",
        "  # --- \n",
        "  \n",
        "  #run['Table_Of_Mean_TTA_Test_Predictions'] = neptune.types.File.as_html(df)\n",
        "  \n",
        "  df.to_csv('df.csv', sep = ',')\n",
        "  variable_name = \"CSV_\" + str(rot_degree)\n",
        "  run[variable_name].upload(\"df.csv\")  \n",
        "\n",
        "  return nll_tta, pred_std_tta, entropy_tta, correct_idx"
      ],
      "metadata": {
        "id": "9gf4iPfa0djG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For-Loop  i = 1 through 5\n",
        "\n",
        "# Define a Run Function\n",
        "\n",
        "\n",
        "# create a dictionary of  parameters to pass to data_generator\n",
        "for rot_degree in range(1,10,2):\n",
        "  run = new_run()\n",
        "\n",
        "  print(rot_degree)\n",
        "\n",
        "\n",
        "  init_params = make_init_params(tta_steps = 5, high_rotation=rot_degree)  # I need an inbetween var bc when run.stop runs the run[parameters] var disappears \n",
        "\n",
        "  run[\"parameters\"]  = init_params\n",
        "\n",
        "  test_datagen = make_image_data_generator(init_params)\n",
        "\n",
        "  # save predictions\n",
        "  final_pred = make_predictions_with_tta()\n",
        "  nll_tta, pred_std_tta, entropy_tta, correct_idx  = calc_metrics()\n",
        "\n",
        "  # delete_old_params()\n",
        "\n",
        "\n",
        "    \n",
        "  def plot_hists(dist, title, xlabel, xlim=None):\n",
        "      plt.hist(dist[unknown_idx],bins=30, density=True,alpha = 0.7)\n",
        "      plt.hist(dist[known_idx],bins=30,  density=True,alpha = 0.7)\n",
        "      plt.hist(dist[correct_idx],bins=30,  density=True,alpha = 0.7)\n",
        "      plt.title(title)\n",
        "      plt.legend(['unknown','known', 'known_correct'])\n",
        "      plt.xlabel(xlabel)\n",
        "      if xlim != None:\n",
        "          plt.xlim(xlim)\n",
        "  plt.figure(figsize=(10,10))\n",
        "\n",
        "  # Non-Bayesian\n",
        "  plt.subplot(4,3,1)\n",
        "  plot_hists(nll, \"Non-Bayesian nll of max p\", \"NLL\", [-0.2,2])\n",
        "  plt.subplot(4,3,2)\n",
        "  plt.axis(\"off\")\n",
        "  plt.subplot(4,3,3)\n",
        "  plot_hists(entropy, \"Non-Bayesian entropy\", \"Entropy\", [-0.2,2])\n",
        "\n",
        "  # # VI\n",
        "  # plt.subplot(4,3,4)\n",
        "  # plot_hists(nll_vi, \"VI nll of max p\", \"NLL\", [-0.2,2])\n",
        "  # plt.subplot(4,3,5)\n",
        "  # plot_hists(pred_std_vi, \"VI std dev.\", \"std\", [-0.2,0.8])\n",
        "  # plt.subplot(4,3,6)\n",
        "  # plot_hists(entropy_vi, \"VI entropy\", \"Entropy\", [-0.2,3.2])\n",
        "\n",
        "  # # MC Methods\n",
        "  # plt.subplot(4,3,7)\n",
        "  # plot_hists(nll_mc, \"MC nll of max p\", \"NLL\", [-0.2,2])\n",
        "  # plt.subplot(4,3,8)\n",
        "  # plot_hists(pred_std_mc, \"MC std dev.\", \"std\", [-0.2,0.8])\n",
        "  # plt.subplot(4,3,9)\n",
        "  # plot_hists(entropy_mc, \"MC entropy\", \"Entropy\", [-0.2,3.2])\n",
        "\n",
        "\n",
        "\n",
        "  # w/TTA Methods\n",
        "  plt.subplot(4,3,4)\n",
        "  plot_hists(nll_tta, \"TTA nll of max p\", \"NLL\", [-0.2,2])\n",
        "  plt.subplot(4,3,5)\n",
        "  plot_hists(pred_std_tta, \"TTA std dev.\", \"std\", [-0.2,0.8])\n",
        "  plt.subplot(4,3,6)\n",
        "  plot_hists(entropy_tta, \"TTA entropy\", \"Entropy\", [-0.2,3.2])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  plt.show()\n",
        "  \n",
        "  run.stop()\n"
      ],
      "metadata": {
        "id": "7pu0ZplQJXKB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run[\"val_cm\"] = val_cm                    #This should never change unless the vanilla model changes\n",
        "# run[\"test_acc_all\"] = test_acc_all        #This should never change unless the vanilla model changes\n",
        "# run[\"test_acc_known\"] = test_acc_known    #This should never change unless the vanilla model changes\n",
        "# run[\"test_cm\"] = cm                       #This should never change unless the vanilla model changes\n",
        "# run[\"true_labels\"] = true_labels        #This should never change unless Train/Val/Test Split changes on Test Set\n",
        "\n",
        "# # ~~~~~~~~~~~~~~~~~~~~~~~~ TTA Specific Metrics to log: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "# # run[\"test_acc_all_tta\"] = test_acc_all_tta\n",
        "# # run[\"test_acc_known_tta\"] = test_acc_known_tta\n",
        "\n",
        "# # run[\"pred_labels_tta\"] = pred_labels_tta\n",
        "# # run[\"nll_tta\"] = nll_tta\n",
        "# # run[\"entropy_tta\"] = entropy_tta\n",
        "# # run[\"pred_max_p_tta\"] = pred_max_p_tta\n",
        "# # run[\"test_cm_tta\"] = cm_tta\n",
        "# # run['predictions_tta_dataframe'] = df\n",
        "\n",
        "\n",
        "# # run[\"val/conf_matrix\"].upload(\"confusion_matrix.png\")\n",
        "# # run[\"TTA_Conf_Matrix_Plot\"].upload(disp_tta)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Jcj8LEhbM49z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Confusion Matrix for TTA "
      ],
      "metadata": {
        "id": "R5I85jNb_T01"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# cm_tta = confusion_matrix(true_labels, pred_labels_tta)\n",
        "# disp_tta = ConfusionMatrixDisplay(confusion_matrix=cm_tta, display_labels=labels)\n",
        "# disp_tta.plot(cmap=plt.cm.Blues)\n",
        "# plt.show()\n",
        "# neptune.types.File.as_image(disp_tta)"
      ],
      "metadata": {
        "id": "IcPsZmt8-sgc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "See Misclassified Images"
      ],
      "metadata": {
        "id": "P6JKa1TginlD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " "
      ],
      "metadata": {
        "id": "h9_69EFvNMFi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "m1dcQj2-Q6MJ"
      },
      "cell_type": "markdown",
      "source": [
        "### See the impact of TTA on a single image"
      ]
    },
    {
      "metadata": {
        "id": "cEA8OgcBQ6MJ"
      },
      "cell_type": "code",
      "source": [
        "image_ix = 1\n",
        "test_image = x_val[image_ix]\n",
        "plt.imshow(revertscale_fn(test_image)/255)\n",
        "plt.title(f'real label: {np.argmax(y_val[image_ix])}')\n",
        "plt.axis('off');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "02h9Og0joX63"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3Zt-DoKJQ6MP"
      },
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(20,14))\n",
        "predictions_tta_idx = []\n",
        "\n",
        "for i in range(5):\n",
        "    im = test_datagen.flow(test_image.reshape(1,32,32,3), batch_size=1, shuffle=False)\n",
        "    im = next(im)\n",
        "    pred = model.predict(im.reshape(1,32,32,3))\n",
        "    predictions_tta_idx.append(pred)\n",
        "    plt.subplot(1, 5, i+1)\n",
        "    plt.title(f'pred label: {np.argmax(pred)}')\n",
        "    im = (revertscale_fn(im)/255)\n",
        "\n",
        "    plt.imshow(im.reshape(32, 32, 3), cmap=plt.get_cmap('gray'))\n",
        "    plt.axis('off')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ozb5T7DqQ6MV"
      },
      "cell_type": "code",
      "source": [
        "final_pred_idx = np.mean(predictions_tta_idx, axis=0)\n",
        "print(f'final pred: {np.argmax(final_pred_idx)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "now = datetime.datetime.now()\n",
        "\n",
        "# now.strftime('%Y-%m-%d %H:%M:%S')\n",
        "\n",
        "print(now)\n",
        "\n",
        "print(now-start)"
      ],
      "metadata": {
        "id": "k7i8BLiJIuTB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compare the uncertainty measures for all known and unknown classes\n",
        "\n",
        " Let's compare the uncertainty measures for all images and all three models. Look at the distributions for the known and unknown class. Can you see a difference? You will look at the nll the total standard deviation and the entropy. Note that there is no total standard deviation for the non-bayesian network, because the prediction is always the same, even if you predict the same image for multiple times.\n",
        "\n",
        "### Unknown and known classes"
      ],
      "metadata": {
        "id": "I3VZbfgURc5l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# def plot_hists(dist, title, xlabel, xlim=None):\n",
        "#     plt.hist(dist[unknown_idx],bins=30, density=True,alpha = 0.7)\n",
        "#     plt.hist(dist[known_idx],bins=30,  density=True,alpha = 0.7)\n",
        "#     plt.title(title)\n",
        "#     plt.legend(['unknown','known'])\n",
        "#     plt.xlabel(xlabel)\n",
        "#     if xlim != None:\n",
        "#         plt.xlim(xlim)\n",
        "# plt.figure(figsize=(18,18))\n",
        "\n",
        "# # Non-Bayesian\n",
        "# plt.subplot(4,3,1)\n",
        "# plot_hists(nll_, \"Non-Bayesian nll of max p\", \"NLL\", [-0.2,2])\n",
        "# plt.subplot(4,3,2)\n",
        "# plt.axis(\"off\")\n",
        "# plt.subplot(4,3,3)\n",
        "# plot_hists(entropy, \"Non-Bayesian entropy\", \"Entropy\", [-0.2,2])\n",
        "\n",
        "# # # VI\n",
        "# # plt.subplot(4,3,4)\n",
        "# # plot_hists(nll_vi, \"VI nll of max p\", \"NLL\", [-0.2,2])\n",
        "# # plt.subplot(4,3,5)\n",
        "# # plot_hists(pred_std_vi, \"VI std dev.\", \"std\", [-0.2,0.8])\n",
        "# # plt.subplot(4,3,6)\n",
        "# # plot_hists(entropy_vi, \"VI entropy\", \"Entropy\", [-0.2,3.2])\n",
        "\n",
        "# # # MC Methods\n",
        "# # plt.subplot(4,3,7)\n",
        "# # plot_hists(nll_mc, \"MC nll of max p\", \"NLL\", [-0.2,2])\n",
        "# # plt.subplot(4,3,8)\n",
        "# # plot_hists(pred_std_mc, \"MC std dev.\", \"std\", [-0.2,0.8])\n",
        "# # plt.subplot(4,3,9)\n",
        "# # plot_hists(entropy_mc, \"MC entropy\", \"Entropy\", [-0.2,3.2])\n",
        "\n",
        "# # w/TTA Methods\n",
        "# plt.subplot(4,3,4)\n",
        "# plot_hists(nll_tta, \"TTA nll of max p\", \"NLL\", [-0.2,2])\n",
        "# plt.subplot(4,3,5)\n",
        "# plot_hists(pred_std_tta, \"TTA std dev.\", \"std\", [-0.2,0.8])\n",
        "# plt.subplot(4,3,6)\n",
        "# plot_hists(entropy_tta, \"TTA entropy\", \"Entropy\", [-0.2,3.2])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "wlhWaeu2RZ_L"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}